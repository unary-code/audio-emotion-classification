{"cells":[{"cell_type":"code","execution_count":null,"id":"69638c27","metadata":{"id":"69638c27","executionInfo":{"status":"error","timestamp":1746472835013,"user_tz":300,"elapsed":16574,"user":{"displayName":"Freya Zhang","userId":"00987220687967084175"}},"outputId":"6a1134e1-852c-4a9d-ca84-c0620e7e4b23","colab":{"base_uri":"https://localhost:8080/","height":365}},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f7d4a8b3b484>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioEmotionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioEmotionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import librosa\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from IPython.display import clear_output\n","\n","# Emotion labels\n","labels = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Suprised']\n","label_to_index = {label: i for i, label in enumerate(labels)}\n","\n","# Dataset class\n","class AudioEmotionDataset(Dataset):\n","    def __init__(self, file_paths, sr=22050, max_len=5.0, n_mels=128):\n","        self.file_paths = file_paths\n","        self.sr = sr\n","        self.n_mels = n_mels\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        path, label = self.file_paths[idx]\n","        y, sr = librosa.load(path, sr=self.sr, duration=self.max_len)\n","        if len(y) < int(self.max_len * sr):\n","            y = np.pad(y, (0, int(self.max_len * sr) - len(y)))\n","\n","        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels)\n","        mel_db = librosa.power_to_db(mel, ref=np.max)\n","        mel_tensor = torch.tensor(mel_db).unsqueeze(0)  # [1, 128, T]\n","\n","        return mel_tensor.float(), torch.tensor(label)\n","\n","# Prepare stratified split file paths\n","from collections import defaultdict\n","all_file_paths = []\n","dataset_path = \"elec378 sp25 dataset\"\n","for label in labels:\n","    folder = os.path.join(dataset_path, label)\n","    if not os.path.isdir(folder):\n","        continue\n","    for file in os.listdir(folder):\n","        if file.endswith(\".wav\"):\n","            all_file_paths.append((os.path.join(folder, file), label_to_index[label]))\n","\n","by_label = defaultdict(list)\n","for path, label in all_file_paths:\n","    by_label[label].append((path, label))\n","\n","train_data, val_data = [], []\n","for label, items in by_label.items():\n","    train_split, val_split = train_test_split(items, test_size=0.2, random_state=42)\n","    train_data.extend(train_split)\n","    val_data.extend(val_split)\n","\n","train_dataset = AudioEmotionDataset(train_data)\n","val_dataset = AudioEmotionDataset(val_data)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16)\n","\n","# Dynamic CNN\n","class AudioCNN(nn.Module):\n","    def __init__(self, num_classes=7, filters=[16, 32, 64], fc_dim=128, dropout=0.3):\n","        super(AudioCNN, self).__init__()\n","        self.conv_block1 = nn.Sequential(\n","            nn.Conv2d(1, filters[0], kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(filters[0]),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(2, 2))\n","        )\n","        self.conv_block2 = nn.Sequential(\n","            nn.Conv2d(filters[0], filters[1], kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(filters[1]),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(2, 2))\n","        )\n","        self.conv_block3 = nn.Sequential(\n","            nn.Conv2d(filters[1], filters[2], kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(filters[2]),\n","            nn.ReLU(),\n","            nn.AdaptiveMaxPool2d((4, 4))\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(filters[2] * 4 * 4, fc_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(fc_dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_block1(x)\n","        x = self.conv_block2(x)\n","        x = self.conv_block3(x)\n","        x = self.fc(x)\n","        return x\n","\n","# Train once function\n","\n","def train_one(model, train_loader, val_loader, epochs=5, lr=0.001):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(1, epochs + 1):\n","        model.train()\n","        total_loss = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        print(f\"Epoch {epoch}/{epochs} - Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","    return total_loss, val_loss, model.state_dict()\n","\n","# Architecture search\n","architectures = [\n","     # Around best: 256, 0.2\n","    {\"filters\": [16, 32, 64], \"fc_dim\": 1098, \"dropout\": 0.25},\n","\n","\n","]\n","\n","results = []\n","for arch in architectures:\n","    print(\"Testing architecture:\", arch)\n","    model = AudioCNN(num_classes=7, **arch)\n","    train_loss, val_loss, weights = train_one(model, train_loader, val_loader, epochs=2)\n","    results.append((arch, val_loss, weights))\n","\n","# Choose best architecture\n","best_arch, best_val_loss, best_weights = min(results, key=lambda x: x[1])\n","print(\"Best architecture:\", best_arch, \"with val loss:\", best_val_loss)\n","\n","# Retrain best architecture with early stopping\n","\n","def train_with_early_stopping(model, train_loader, val_loader, max_epochs=20, lr=0.001):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for epoch in range(1, max_epochs + 1):\n","        model.train()\n","        total_loss = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        clear_output(wait=True)\n","        print(f\"Epoch {epoch}/{max_epochs} - Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model.state_dict()\n","        else:\n","            print(\"Validation loss increased. Stopping early and keeping best model.\")\n","            print(f\"Previous best: {best_val_loss:.4f} | Current val: {val_loss:.4f}\")\n","            break\n","\n","    if best_model:\n","        torch.save(best_model, \"audio_emotion_model.pth\")\n","        print(\" Best model saved as audio_emotion_model.pth\")\n","\n","# Final run\n","final_model = AudioCNN(num_classes=7, **best_arch)\n","train_with_early_stopping(final_model, train_loader, val_loader, max_epochs=20)\n"]},{"cell_type":"code","execution_count":null,"id":"5b871633","metadata":{"id":"5b871633","outputId":"f041a5cd-52fd-4936-a59c-7de976c0ce6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 3/20 - Train Loss: 629.8437 | Val Loss: 154.8518\n"]}],"source":["model = AudioCNN(num_classes=7, filters=[16, 32, 64], fc_dim=1098, dropout=0.25)\n","\n","# train set archeticture\n","train_with_early_stopping(model, train_loader, val_loader, max_epochs=20)"]},{"cell_type":"code","execution_count":null,"id":"0afb11ef","metadata":{"id":"0afb11ef","outputId":"6241824a-8ec6-45a5-bef4-7138fbc32db6"},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Saved predictions to submission.csv!\n"]}],"source":["import os\n","import torch\n","import librosa\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Emotion labels\n","labels = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Suprised']\n","index_to_label = {i: label for i, label in enumerate(labels)}\n","best_arch = {\"filters\": [16, 32, 64], \"fc_dim\": 1098, \"dropout\": 0.25}  # slightly more\n","# Load the trained model\n","model = AudioCNN(num_classes=7, **best_arch)\n","model.load_state_dict(torch.load(\"audio_emotion_model.pth\", map_location=torch.device('cpu')))\n","model.eval()\n","\n","\n","# Test Dataset\n","class TestAudioDataset(Dataset):\n","    def __init__(self, test_dir, sr=22050, max_len=5.0, n_mels=128):\n","        self.file_paths = []\n","        self.sr = sr\n","        self.n_mels = n_mels\n","        self.max_len = max_len\n","\n","        # Sort files numerically: 1.wav, 2.wav, ..., 10.wav\n","        for file in sorted(os.listdir(test_dir), key=lambda x: int(os.path.splitext(x)[0])):\n","            if file.endswith(\".wav\"):\n","                self.file_paths.append(os.path.join(test_dir, file))\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        path = self.file_paths[idx]\n","        y, sr = librosa.load(path, sr=self.sr, duration=self.max_len)\n","        if len(y) < int(self.max_len * sr):\n","            y = np.pad(y, (0, int(self.max_len * sr) - len(y)))\n","\n","        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels)\n","        mel_db = librosa.power_to_db(mel, ref=np.max)\n","        mel_tensor = torch.tensor(mel_db).unsqueeze(0).float()  # [1, 128, T]\n","\n","        filename = os.path.basename(path)\n","        return mel_tensor, filename\n","\n","\n","test_path = \"elec378 sp25 dataset/Test\"  # <-- test path\n","test_dataset = TestAudioDataset(test_path)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","# Run inference\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","predictions = []\n","\n","with torch.no_grad():\n","    for mel_tensor, filename in test_loader:\n","        mel_tensor = mel_tensor.to(device)\n","        output = model(mel_tensor)\n","        predicted_index = torch.argmax(output, dim=1).item()\n","        predicted_emotion = index_to_label[predicted_index]\n","        predictions.append((filename[0], predicted_emotion))\n","\n","# Create CSV\n","submission_df = pd.DataFrame(predictions, columns=[\"name\", \"emotion\"])\n","submission_df.to_csv(\"submission.csv\", index=False)\n","print(\" Saved predictions to submission.csv\")\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}